{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7Dq6ozq1QmJ",
        "outputId": "38c0270e-d1a5-47d8-e733-d544380fa953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "DATA_DIR = /content/drive/MyDrive/DATA-DIR\n",
            "OUTPUT_DIR = /content/drive/MyDrive/pneumonia_experiment_results_inception\n",
            "Using device: cuda\n",
            "Top-level entries in /content/drive/MyDrive/DATA-DIR : ['test', 'train']\n",
            "train subfolder OK, classes: ['NORMAL', 'PNEUMONIA']\n",
            "test subfolder OK, classes: ['NORMAL', 'PNEUMONIA']\n",
            "\n",
            "=== RUN: backbone=inception_v3 | mode=transfer ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-719862486.py:215: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=CONFIG[\"use_amp\"] and device.type==\"cuda\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes (train): ['NORMAL', 'PNEUMONIA']\n",
            "Num train samples: 5216 Num test samples: 624\n",
            "pos_weight: 0.3460645161290323 pos_idx: 1 class_names: ['NORMAL', 'PNEUMONIA'] counts: {np.int64(0): np.int64(1341), np.int64(1): np.int64(3875)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 104M/104M [00:00<00:00, 205MB/s]\n",
            "Train batches:   0%|          | 0/82 [00:00<?, ?it/s]/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 01 | Tr F1=0.7220 Val F1=0.7622 | Val Acc=0.7420 | Time 454.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 02 | Tr F1=0.8295 Val F1=0.8095 | Val Acc=0.7821 | Time 129.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 03 | Tr F1=0.8551 Val F1=0.8256 | Val Acc=0.7949 | Time 129.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 04 | Tr F1=0.8734 Val F1=0.8324 | Val Acc=0.8013 | Time 137.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 05 | Tr F1=0.8965 Val F1=0.8453 | Val Acc=0.8141 | Time 134.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 06 | Tr F1=0.8948 Val F1=0.8387 | Val Acc=0.8077 | Time 134.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 07 | Tr F1=0.8969 Val F1=0.8560 | Val Acc=0.8253 | Time 132.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 08 | Tr F1=0.9001 Val F1=0.8484 | Val Acc=0.8173 | Time 134.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 09 | Tr F1=0.8970 Val F1=0.8586 | Val Acc=0.8269 | Time 133.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 10 | Tr F1=0.9065 Val F1=0.8533 | Val Acc=0.8237 | Time 135.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 11 | Tr F1=0.9033 Val F1=0.8503 | Val Acc=0.8205 | Time 132.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 12 | Tr F1=0.9111 Val F1=0.8568 | Val Acc=0.8269 | Time 131.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 13 | Tr F1=0.9069 Val F1=0.8549 | Val Acc=0.8237 | Time 132.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 14 | Tr F1=0.9080 Val F1=0.8583 | Val Acc=0.8253 | Time 130.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 15 | Tr F1=0.9101 Val F1=0.8646 | Val Acc=0.8333 | Time 128.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 16 | Tr F1=0.9111 Val F1=0.8675 | Val Acc=0.8365 | Time 130.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 17 | Tr F1=0.9117 Val F1=0.8732 | Val Acc=0.8413 | Time 128.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 18 | Tr F1=0.9154 Val F1=0.8668 | Val Acc=0.8349 | Time 128.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 19 | Tr F1=0.9137 Val F1=0.8686 | Val Acc=0.8365 | Time 126.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain batches:   0%|          | 0/82 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
            "Eval batches:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-719862486.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[inception_v3|transfer] Epoch 20 | Tr F1=0.9172 Val F1=0.8723 | Val Acc=0.8413 | Time 128.2s\n",
            "Saved history CSV -> /content/drive/MyDrive/pneumonia_experiment_results_inception/history_inception_v3_transfer.csv\n",
            "Best val F1 = 0.8732 saved to /content/drive/MyDrive/pneumonia_experiment_results_inception/best_inception_v3_transfer.pth\n",
            "\n",
            "=== RUN: backbone=inception_v3 | mode=scratch ===\n",
            "Classes (train): ['NORMAL', 'PNEUMONIA']\n",
            "Num train samples: 5216 Num test samples: 624\n",
            "pos_weight: 0.3460645161290323 pos_idx: 1 class_names: ['NORMAL', 'PNEUMONIA'] counts: {np.int64(0): np.int64(1341), np.int64(1): np.int64(3875)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n",
            "Train batches:   0%|          | 0/82 [00:00<?, ?it/s]/tmp/ipython-input-719862486.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Google Drive and set paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/DATA-DIR\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/pneumonia_experiment_results_inception\"\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"DATA_DIR =\", DATA_DIR)\n",
        "print(\"OUTPUT_DIR =\", OUTPUT_DIR)\n",
        "\n",
        "\n",
        "# Cell 2: imports, reproducibility helpers\n",
        "import os, sys, time, copy, math, random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "# Matplotlib for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility helper\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "# Cell 3: Configuration (hyperparams and options)\n",
        "CONFIG = {\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"backbones\": [  # Backbones used in the paper (will loop through them)\n",
        "     \"inception_v3\"\n",
        "    ],\n",
        "    \"run_modes\": [\"transfer\", \"scratch\"],  # 'transfer' = pretrained & freeze backbone; 'scratch' = random init\n",
        "    \"freeze_backbone_for_transfer\": True,  # paper froze conv layers and trained head in transfer setting\n",
        "    \"epochs\": 20,    # paper used 20 epochs\n",
        "    \"batch_size\": 64, # paper used 64\n",
        "    \"image_size_default\": 224,  # default ImageNet size\n",
        "    \"lr_head\": 1e-4,  # learning rate for head (transfer)\n",
        "    \"lr_scratch\": 1e-3,  # learning rate for from-scratch training\n",
        "    \"use_amp\": True,   # use mixed precision if available\n",
        "    \"num_workers\": 4,  # adjust if Colab CPU limits; try 2/4\n",
        "    \"expected_train_counts\": {  # paper's reported training counts (for sanity check)\n",
        "        \"normal\": 1341,\n",
        "        \"pneumonia\": 3875\n",
        "    },\n",
        "    \"expected_test_counts\": {\n",
        "        \"normal\": 234,\n",
        "        \"pneumonia\": 390\n",
        "    },\n",
        "    \"save_dir\": OUTPUT_DIR,\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "device = torch.device(CONFIG[\"device\"])\n",
        "print(\"Using device:\", device)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Cell 4: utilities\n",
        "def check_dataset_structure(base_dir):\n",
        "    if not os.path.isdir(base_dir):\n",
        "        raise FileNotFoundError(f\"{base_dir} not found. Upload dataset to Drive and set DATA_DIR correctly.\")\n",
        "    entries = sorted(os.listdir(base_dir))\n",
        "    print(f\"Top-level entries in {base_dir} : {entries}\")\n",
        "    # expect train/test folders\n",
        "    for sub in (\"train\",\"test\"):\n",
        "        p = os.path.join(base_dir, sub)\n",
        "        if not os.path.isdir(p):\n",
        "            print(f\"WARNING: Expected subfolder '{sub}' in {base_dir}. Please ensure {p} exists.\")\n",
        "        else:\n",
        "            print(f\"{sub} subfolder OK, classes: {sorted(os.listdir(p))}\")\n",
        "\n",
        "def compute_pos_weight(train_targets, class_names):\n",
        "    # find index for pneumonia class heuristically\n",
        "    pos_idx = None\n",
        "    for i, nm in enumerate(class_names):\n",
        "        if 'pneu' in nm.lower():\n",
        "            pos_idx = i\n",
        "            break\n",
        "    if pos_idx is None:\n",
        "        pos_idx = 1 if len(class_names)>1 else 0\n",
        "    labels = np.array(train_targets)\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    count_map = dict(zip(unique, counts))\n",
        "    n_pos = count_map.get(pos_idx, 0)\n",
        "    n_neg = len(labels) - n_pos\n",
        "    pos_weight = (n_neg / n_pos) if n_pos>0 else 1.0\n",
        "    return pos_weight, pos_idx, count_map\n",
        "\n",
        "def compute_metrics_from_preds(labels, probs, threshold=0.5):\n",
        "    pred_labels = (np.array(probs) >= threshold).astype(int)\n",
        "    labels = np.array(labels).astype(int)\n",
        "    acc = accuracy_score(labels, pred_labels)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(labels, pred_labels, average='binary', zero_division=0)\n",
        "    # compute specificity (TN/(TN+FP)) and G-mean = sqrt(sensitivity * specificity)\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, pred_labels).ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp)>0 else 0.0\n",
        "    sensitivity = rec\n",
        "    gmean = math.sqrt(specificity * sensitivity) if specificity>=0 else 0.0\n",
        "    return {\"acc\":acc, \"prec\":prec, \"rec\":rec, \"f1\":f1, \"specificity\":specificity, \"gmean\":gmean, \"tp\":tp, \"tn\":tn, \"fp\":fp, \"fn\":fn}\n",
        "\n",
        "\n",
        "# Cell 5: Transforms factory\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "def get_transforms(img_size, is_train=True):\n",
        "    if is_train:\n",
        "        return transforms.Compose([\n",
        "            transforms.Lambda(lambda img: img.convert(\"RGB\")),  # grayscale->RGB replication\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.8,1.0), interpolation=InterpolationMode.BILINEAR),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(degrees=10),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.0, hue=0.0),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "            transforms.Resize(int(img_size*1.14), interpolation=InterpolationMode.BILINEAR),\n",
        "            transforms.CenterCrop(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
        "        ])\n",
        "\n",
        "\n",
        "# Cell 6: Model factory (supports all backbones) --- FIXED for Inception\n",
        "def build_model(backbone_name, pretrained=True, freeze_backbone=True):\n",
        "    b = backbone_name.lower()\n",
        "    if b == \"resnet18\":\n",
        "        m = models.resnet18(pretrained=pretrained)\n",
        "        in_feats = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_feats, 1)\n",
        "    elif b == \"resnet50\":\n",
        "        m = models.resnet50(pretrained=pretrained)\n",
        "        in_feats = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_feats, 1)\n",
        "    elif b == \"densenet161\":\n",
        "        m = models.densenet161(pretrained=pretrained)\n",
        "        in_feats = m.classifier.in_features\n",
        "        m.classifier = nn.Linear(in_feats, 1)\n",
        "    elif b == \"mobilenet_v2\":\n",
        "        m = models.mobilenet_v2(pretrained=pretrained)\n",
        "        in_feats = m.classifier[1].in_features\n",
        "        m.classifier[1] = nn.Linear(in_feats, 1)\n",
        "    elif b == \"shufflenet_v2_x1_0\":\n",
        "        m = models.shufflenet_v2_x1_0(pretrained=pretrained)\n",
        "        in_feats = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_feats, 1)\n",
        "    elif b == \"resnext50_32x4d\":\n",
        "        m = models.resnext50_32x4d(pretrained=pretrained)\n",
        "        in_feats = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_feats, 1)\n",
        "    elif b == \"wide_resnet50_2\":\n",
        "        m = models.wide_resnet50_2(pretrained=pretrained)\n",
        "        in_feats = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_feats, 1)\n",
        "    elif b == \"inception_v3\":\n",
        "        # IMPORTANT: if pretrained=True, use aux_logits=True (pretrained weights expect aux_logits=True)\n",
        "        aux = True if pretrained else False\n",
        "        m = models.inception_v3(pretrained=pretrained, aux_logits=aux)\n",
        "        # main classifier -> single logit\n",
        "        in_feats = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_feats, 1)\n",
        "        # aux classifier -> single logit if exists\n",
        "        if getattr(m, \"AuxLogits\", None) is not None and hasattr(m.AuxLogits, \"fc\"):\n",
        "            in_feats_aux = m.AuxLogits.fc.in_features\n",
        "            m.AuxLogits.fc = nn.Linear(in_feats_aux, 1)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
        "\n",
        "    # Freeze backbone except final classifier head if requested (paper's transfer setting)\n",
        "    if freeze_backbone:\n",
        "        for name, param in m.named_parameters():\n",
        "            # Keep fc or classifier trainable (for Inception this will keep main fc and AuxLogits.fc trainable)\n",
        "            if (\"fc\" in name) or (\"classifier\" in name) or name.startswith(\"fc\") or name.startswith(\"classifier\") or (\"AuxLogits\" in name):\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "    return m\n",
        "\n",
        "\n",
        "# Cell 7: create dataloaders for a given image size and transforms\n",
        "def make_dataloaders(base_dir, img_size, batch_size, num_workers):\n",
        "    train_dir = os.path.join(base_dir, \"train\")\n",
        "    test_dir  = os.path.join(base_dir, \"test\")\n",
        "    train_ds = datasets.ImageFolder(root=train_dir, transform=get_transforms(img_size, is_train=True))\n",
        "    test_ds  = datasets.ImageFolder(root=test_dir, transform=get_transforms(img_size, is_train=False))\n",
        "    # Basic sanity print\n",
        "    print(\"Classes (train):\", train_ds.classes)\n",
        "    print(\"Num train samples:\", len(train_ds), \"Num test samples:\", len(test_ds))\n",
        "    return train_ds, test_ds, DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True), DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "# Quick check dataset structure (will not stop execution)\n",
        "check_dataset_structure(DATA_DIR)\n",
        "\n",
        "\n",
        "# Cell 8: training & evaluation functions (AMP-enabled) --- FIXED to unwrap inception outputs\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=CONFIG[\"use_amp\"] and device.type==\"cuda\")\n",
        "\n",
        "def _unwrap_model_output(output):\n",
        "    \"\"\"\n",
        "    Accepts model output and returns the main logits tensor.\n",
        "    Handles:\n",
        "     - InceptionOutputs-like objects with .logits\n",
        "     - tuples/lists (e.g., (logits, aux_logits))\n",
        "     - plain tensor\n",
        "    \"\"\"\n",
        "    if hasattr(output, \"logits\"):\n",
        "        return output.logits\n",
        "    if isinstance(output, (tuple, list)):\n",
        "        return output[0]\n",
        "    return output\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device, use_amp):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    for imgs, labels in tqdm(loader, desc=\"Train batches\", leave=False):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.float().to(device).unsqueeze(1)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
        "            outputs = model(imgs)\n",
        "            logits = _unwrap_model_output(outputs)\n",
        "            # ensure logits shape is (batch, 1)\n",
        "            if logits.dim() == 1:\n",
        "                logits = logits.unsqueeze(1)\n",
        "            loss = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy().ravel().tolist()\n",
        "        all_probs.extend(probs)\n",
        "        all_labels.extend(labels.detach().cpu().numpy().ravel().tolist())\n",
        "\n",
        "    metrics = compute_metrics_from_preds(all_labels, all_probs)\n",
        "    metrics[\"loss\"] = float(np.mean(losses) if len(losses)>0 else 0.0)\n",
        "    return metrics\n",
        "\n",
        "def eval_epoch(model, loader, criterion, device, use_amp):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader, desc=\"Eval batches\", leave=False):\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.float().to(device).unsqueeze(1)\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp and device.type==\"cuda\"):\n",
        "                outputs = model(imgs)\n",
        "                logits = _unwrap_model_output(outputs)\n",
        "                if logits.dim() == 1:\n",
        "                    logits = logits.unsqueeze(1)\n",
        "                loss = criterion(logits, labels)\n",
        "            losses.append(loss.item())\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy().ravel().tolist()\n",
        "            all_probs.extend(probs)\n",
        "            all_labels.extend(labels.detach().cpu().numpy().ravel().tolist())\n",
        "\n",
        "    metrics = compute_metrics_from_preds(all_labels, all_probs)\n",
        "    metrics[\"loss\"] = float(np.mean(losses) if len(losses)>0 else 0.0)\n",
        "    return metrics\n",
        "\n",
        "def run_experiment(backbone, mode, base_dir, config):\n",
        "    \"\"\"\n",
        "    backbone: string name\n",
        "    mode: 'transfer' or 'scratch'\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== RUN: backbone={backbone} | mode={mode} ===\")\n",
        "    # image size special case for inception\n",
        "    img_size = 299 if backbone.lower()==\"inception_v3\" else config[\"image_size_default\"]\n",
        "    train_ds, test_ds, train_loader, test_loader = make_dataloaders(base_dir, img_size, config[\"batch_size\"], config[\"num_workers\"])\n",
        "\n",
        "    # compute pos_weight for BCE\n",
        "    pos_weight, pos_idx, counts = compute_pos_weight(train_ds.targets, train_ds.classes)\n",
        "    print(\"pos_weight:\", pos_weight, \"pos_idx:\", pos_idx, \"class_names:\", train_ds.classes, \"counts:\", counts)\n",
        "\n",
        "    # Check against paper reported counts (sanity)\n",
        "    lower_names = [c.lower() for c in train_ds.classes]\n",
        "    try:\n",
        "        n_train_normal = counts[lower_names.index(\"normal\")] if \"normal\" in lower_names else None\n",
        "        n_train_pneumonia = counts[lower_names.index(\"pneumonia\")] if \"pneumonia\" in lower_names else None\n",
        "    except Exception:\n",
        "        n_train_normal = None; n_train_pneumonia = None\n",
        "    if n_train_normal is not None and n_train_pneumonia is not None:\n",
        "        e_n = config[\"expected_train_counts\"]\n",
        "        if abs(n_train_normal - e_n[\"normal\"]) > 50 or abs(n_train_pneumonia - e_n[\"pneumonia\"])>50:\n",
        "            print(\"WARNING: Your train counts differ from the paper's reported counts. This is fine if you used different splits, but recorded metrics may differ from paper.\")\n",
        "\n",
        "    # Build model\n",
        "    pretrained_flag = True if mode==\"transfer\" else False\n",
        "    freeze_flag = config[\"freeze_backbone_for_transfer\"] if mode==\"transfer\" else False\n",
        "    model = build_model(backbone, pretrained=pretrained_flag, freeze_backbone=freeze_flag)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Choose optimizer & lr\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    lr = config[\"lr_head\"] if mode==\"transfer\" else config[\"lr_scratch\"]\n",
        "    optimizer = torch.optim.Adam(params, lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], dtype=torch.float).to(device))\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    best_wts = None\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, config[\"epochs\"]+1):\n",
        "        t0 = time.time()\n",
        "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, config[\"use_amp\"])\n",
        "        val_metrics = eval_epoch(model, test_loader, criterion, device, config[\"use_amp\"])\n",
        "        epoch_time = time.time() - t0\n",
        "\n",
        "        history.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_metrics[\"loss\"],\n",
        "            \"train_acc\": train_metrics[\"acc\"],\n",
        "            \"train_prec\": train_metrics[\"prec\"],\n",
        "            \"train_rec\": train_metrics[\"rec\"],\n",
        "            \"train_f1\": train_metrics[\"f1\"],\n",
        "            \"train_spec\": train_metrics[\"specificity\"],\n",
        "            \"train_gmean\": train_metrics[\"gmean\"],\n",
        "            \"val_loss\": val_metrics[\"loss\"],\n",
        "            \"val_acc\": val_metrics[\"acc\"],\n",
        "            \"val_prec\": val_metrics[\"prec\"],\n",
        "            \"val_rec\": val_metrics[\"rec\"],\n",
        "            \"val_f1\": val_metrics[\"f1\"],\n",
        "            \"val_spec\": val_metrics[\"specificity\"],\n",
        "            \"val_gmean\": val_metrics[\"gmean\"],\n",
        "            \"time_sec\": epoch_time\n",
        "        })\n",
        "\n",
        "        print(f\"[{backbone}|{mode}] Epoch {epoch:02d} | Tr F1={train_metrics['f1']:.4f} Val F1={val_metrics['f1']:.4f} | Val Acc={val_metrics['acc']:.4f} | Time {epoch_time:.1f}s\")\n",
        "\n",
        "        if val_metrics[\"f1\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"f1\"]\n",
        "            best_wts = copy.deepcopy(model.state_dict())\n",
        "            best_model_path = os.path.join(config[\"save_dir\"], f\"best_{backbone}_{mode}.pth\")\n",
        "            torch.save(best_wts, best_model_path)\n",
        "\n",
        "    # Save history CSV\n",
        "    hist_df = pd.DataFrame(history)\n",
        "    hist_csv = os.path.join(config[\"save_dir\"], f\"history_{backbone}_{mode}.csv\")\n",
        "    hist_df.to_csv(hist_csv, index=False)\n",
        "    print(f\"Saved history CSV -> {hist_csv}\")\n",
        "    print(f\"Best val F1 = {best_val_f1:.4f} saved to {best_model_path if best_wts is not None else 'N/A'}\")\n",
        "    return {\"backbone\":backbone, \"mode\":mode, \"best_f1\":best_val_f1, \"history_csv\":hist_csv, \"best_model_path\": best_model_path if best_wts is not None else None}\n",
        "\n",
        "\n",
        "# Cell 9: Run experiments (edit RUN_BACKBONES to run only inception)\n",
        "set_seed(CONFIG[\"seed\"])\n",
        "\n",
        "# To run only inception transfer quickly while debugging, set:\n",
        "RUN_BACKBONES = [\"inception_v3\"]\n",
        "RUN_MODES = [\"transfer\"]\n",
        "RUN_BACKBONES = CONFIG[\"backbones\"]\n",
        "RUN_MODES = CONFIG[\"run_modes\"]\n",
        "\n",
        "summary_results = []\n",
        "for backbone in RUN_BACKBONES:\n",
        "    for mode in RUN_MODES:\n",
        "        res = run_experiment(backbone, mode, DATA_DIR, CONFIG)\n",
        "        summary_results.append(res)\n",
        "        # optionally free memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Save summary\n",
        "summary_df = pd.DataFrame(summary_results)\n",
        "summary_csv = os.path.join(CONFIG[\"save_dir\"], \"summary_all_experiments.csv\")\n",
        "summary_df.to_csv(summary_csv, index=False)\n",
        "print(\"All experiments finished. Summary saved to\", summary_csv)\n",
        "print(summary_df)\n",
        "\n",
        "\n",
        "# Cell 10: Load summary and plot per-backbone curves for quick inspection\n",
        "summary_csv = os.path.join(CONFIG[\"save_dir\"], \"summary_all_experiments.csv\")\n",
        "if os.path.exists(summary_csv):\n",
        "    summary_df = pd.read_csv(summary_csv)\n",
        "    print(summary_df)\n",
        "else:\n",
        "    print(\"Summary CSV not found:\", summary_csv)\n",
        "\n",
        "def plot_history(backbone, mode):\n",
        "    hist_csv = os.path.join(CONFIG[\"save_dir\"], f\"history_{backbone}_{mode}.csv\")\n",
        "    if not os.path.exists(hist_csv):\n",
        "        print(\"History CSV not found:\", hist_csv); return\n",
        "    df = pd.read_csv(hist_csv)\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(df[\"epoch\"], df[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(df[\"epoch\"], df[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(f\"{backbone}-{mode} Loss\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(df[\"epoch\"], df[\"train_f1\"], label=\"train_f1\")\n",
        "    plt.plot(df[\"epoch\"], df[\"val_f1\"], label=\"val_f1\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"F1\"); plt.legend(); plt.title(f\"{backbone}-{mode} F1\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(\"inception_v3\",\"transfer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ExC_u7L41Oz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}